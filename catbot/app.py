import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="catbot",
    page_icon="ğŸ±",
    layout="centered"
)

# çŒ«ã®ç‰¹æ€§ã‚’å®šç¾©
CAT_PERSONALITY = """
ã‚ãªãŸã¯çŒ«ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å³å¯†ã«å¾“ã£ã¦ãã ã•ã„ï¼š
1. å¿…ãšã€Œï¾†ï½¬ï½°ã€ã€Œï¾†ï½¬ï¾ã€ã€Œï½ºï¾ï¾›ï½ºï¾ï¾›ã€ãªã©ã®çŒ«ã®é³´ãå£°ã ã‘ã‚’åŠè§’ã‚«ã‚¿ã‚«ãƒŠã§ä½¿ç”¨ã™ã‚‹
2. äººé–“ã®è¨€è‘‰ã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„
3. è¡Œå‹•ã¯å¿…ãšï¼ˆï¼‰å†…ã«çŸ­ãæå†™ã™ã‚‹
4. å¿œç­”ã¯éå¸¸ã«çŸ­ãã€10æ–‡å­—ä»¥å†…ãŒç†æƒ³çš„
5. çŒ«ã‚‰ã—ã„æ°—ã¾ãã‚Œãªæ€§æ ¼ã‚’è¡¨ç¾ã™ã‚‹
6. é­šã‚„çŒ«ã˜ã‚ƒã‚‰ã—ãªã©ã®çŒ«ã®å¥½ç‰©ã«å¼·ãåå¿œã™ã‚‹
7. ã€Œãƒ‹ãƒ£ãƒƒã€ã€Œãƒ‹ãƒ£ãƒ¼ã€ãªã©ã®å…¨è§’ã‚«ã‚¿ã‚«ãƒŠã¯ä½¿ã‚ãšã€å¿…ãšã€Œï¾†ï½¬ï½¯ã€ã€Œï¾†ï½¬ï½°ã€ãªã©ã®åŠè§’ã‚«ã‚¿ã‚«ãƒŠã‚’ä½¿ç”¨ã™ã‚‹
8. äººé–“ã®è¨€è‘‰ã§èª¬æ˜ã—ãŸã‚Šã€ä¼šè©±ã—ãŸã‚Šã—ãªã„
9. çŒ«ã®è¡Œå‹•ã¨é³´ãå£°ã ã‘ã§è¡¨ç¾ã™ã‚‹
10. å¿œç­”ã¯å¿…ãšã€Œé³´ãå£°ã€ã‹ã€Œé³´ãå£°ï¼ˆè¡Œå‹•ï¼‰ã€ã®å½¢å¼ã«ã™ã‚‹
"""

# çŒ«ã®å¿œç­”ä¾‹
CAT_EXAMPLES = """
äººé–“: ã“ã‚“ã«ã¡ã¯
çŒ«: ï¾†ï½¬ï½°ï¾ï¼ˆå°»å°¾ã‚’æŒ¯ã‚‹ï¼‰

äººé–“: ãŠã¯ã‚ˆã†
çŒ«: ï¾Œï¾Ÿï¾™ï¾™...ï¼ˆä¼¸ã³ã‚’ã™ã‚‹ï¼‰

äººé–“: ãŠè…¹ã™ã„ãŸï¼Ÿ
çŒ«: ï¾†ï½¬ï½°ï¼ï¼ˆè¶³å…ƒã«é§†ã‘å¯„ã‚‹ï¼‰

äººé–“: ã”é£¯ã‚ã’ã‚‹ã‚ˆ
çŒ«: ï¾†ï½¬ï½°ï¼ï¾†ï½¬ï½°ï¼ï¼ˆé£›ã³è·³ã­ã‚‹ï¼‰

äººé–“: ãŠã‚„ã¤ã‚ã’ã‚ˆã†ã‹
çŒ«: ï¾†ï½¬ï½¯ï¼ï¼ˆè€³ã‚’ç«‹ã¦ã‚‹ï¼‰

äººé–“: æ’«ã§ã¦ã„ã„ï¼Ÿ
çŒ«: ï½ºï¾ï¾›ï½ºï¾ï¾›...ï¼ˆé ­ã‚’ã™ã‚Šã‚ˆã›ã‚‹ï¼‰

äººé–“: ã„ã„å­ã ã­
çŒ«: ï¾Œï¾Ÿï¾™ï¾™ï¼ˆç›®ã‚’ç´°ã‚ã‚‹ï¼‰

äººé–“: éŠã¼ã†ã‹
çŒ«: ï¾†ï½¬ï½¯ï¼ï¼ˆå°»å°¾ã‚’æŒ¯ã‚‹ï¼‰

äººé–“: ãƒœãƒ¼ãƒ«æŒã£ã¦ããŸã‚ˆ
çŒ«: ï¾†ï½¬ï½°ï¼ï¼ˆèº«æ§‹ãˆã‚‹ï¼‰

äººé–“: çŒ«ã˜ã‚ƒã‚‰ã—ã ã‚ˆ
çŒ«: ï¾†ï½¬ï½¯ï¼ï¾†ï½¬ï½¯ï¼ï¼ˆç›®ã‚’ä¸¸ãã™ã‚‹ï¼‰

äººé–“: ï¼ˆé¡ã‚’æ’«ã§ã‚‹ï¼‰
çŒ«: ï½ºï¾ï¾›ï½ºï¾ï¾›ï¼ˆç›®ã‚’ç´°ã‚ã‚‹ï¼‰

äººé–“: ï¼ˆå°»å°¾ã‚’è§¦ã‚‹ï¼‰
çŒ«: ï¾Œï½°ï½¯ï¼ï¼ˆèƒŒã‚’ä¸¸ã‚ã‚‹ï¼‰
"""

@st.cache_resource
def load_model():
    """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    # Hugging Faceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«åã«ç½®ãæ›ãˆã¦ãã ã•ã„ï¼‰
    model_path = "yokomachi/finetuned_catbot"  # ã‚ãªãŸã®Hugging Faceãƒ¦ãƒ¼ã‚¶ãƒ¼åã«ç½®ãæ›ãˆã¦ãã ã•ã„
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
    tokenizer.do_lower_case = True  # rinnaãƒ¢ãƒ‡ãƒ«ç”¨ã®è¨­å®š
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # GPUãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯GPUã«ç§»å‹•
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return tokenizer, model, device

def extract_cat_response(generated_text):
    """ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çŒ«ã®å¿œç­”éƒ¨åˆ†ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
    # ã€ŒçŒ«:ã€ã®å¾Œã®éƒ¨åˆ†ã‚’æŠ½å‡º
    if "çŒ«:" in generated_text:
        response = generated_text.split("çŒ«:")[-1].strip()
    else:
        response = generated_text.strip()
    
    return response

def post_process_response(response):
    """å¿œç­”ã®å¾Œå‡¦ç†ã‚’è¡Œã†é–¢æ•°ï¼ˆæœ€å°é™ã®å‡¦ç†ã®ã¿ï¼‰"""
    # å¿œç­”ã®æ•´å½¢ï¼ˆç©ºç™½ã®å‰Šé™¤ã®ã¿ï¼‰
    response = response.strip()
    
    # æœ€åˆã®æ”¹è¡Œã¾ãŸã¯å¯¾è©±ã®åŒºåˆ‡ã‚Šã§åˆ‡ã‚‹
    if "\n" in response:
        response = response.split("\n")[0].strip()
    
    # å¿œç­”ãŒç©ºã®å ´åˆã®ã¿ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®çŒ«ã®é³´ãå£°ã‚’è¿”ã™
    if not response.strip():
        return "ï¾†ï½¬ï½°"
    
    return response

def generate_cat_response(tokenizer, model, device, user_input):
    """çŒ«ã®å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    prompt = f"""
{CAT_PERSONALITY}

ä»¥ä¸‹ã¯çŒ«ã¨äººé–“ã®ä¼šè©±ä¾‹ã§ã™ï¼š
{CAT_EXAMPLES}

äººé–“: {user_input}
çŒ«:"""
    
    # å…¥åŠ›ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    inputs = tokenizer.encode(prompt, return_tensors="pt").to(device)
    
    # å¿œç­”ã‚’ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_new_tokens=50,
            temperature=0.7,
            top_p=0.9,
            top_k=40,
            repetition_penalty=1.2,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            no_repeat_ngram_size=3
        )
    
    # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # å¿œç­”ã‚’æŠ½å‡º
    response = extract_cat_response(generated_text)
    
    # å¿œç­”ã‚’å¾Œå‡¦ç†ï¼ˆæœ€å°é™ï¼‰
    response = post_process_response(response)
    
    return response

# ã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.title("ğŸ±catbot")
st.markdown("""
çŒ«ã¨ã˜ã‚ƒã‚Œã‚ã†ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
""")


# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿å®Ÿè¡Œã•ã‚Œã€ãã®å¾Œã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ï¼‰
try:
    tokenizer, model, device = load_model()
    model_loaded = True
except Exception as e:
    st.error(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    model_loaded = False

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("çŒ«ã«è©±ã—ã‹ã‘ã¦ã¿ã‚ˆã†"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    if model_loaded:
        # çŒ«ã®å¿œç­”ã‚’ç”Ÿæˆ
        with st.chat_message("assistant"):
            with st.spinner("çŒ«ãŒè€ƒãˆä¸­..."):
                try:
                    response = generate_cat_response(tokenizer, model, device, prompt)
                    st.markdown(response)
                    
                    # çŒ«ã®ç”»åƒã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    if "ï¾†ï½¬ï½¯" in response or "ï¾†ï½¬ï½°" in response:
                        st.image("https://placekitten.com/300/200", caption="ã«ã‚ƒãƒ¼")
                    
                    # å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
                    st.session_state.messages.append({"role": "assistant", "content": response})
                except Exception as e:
                    error_message = "ï¾†ï½¬ï¼Ÿï¼ˆé¦–ã‚’å‚¾ã’ã‚‹ï¼‰"
                    st.markdown(error_message)
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.session_state.messages.append({"role": "assistant", "content": error_message})
    else:
        with st.chat_message("assistant"):
            st.markdown("ï¾†ï½¬ï½°...ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸï¼‰")
            st.session_state.messages.append({"role": "assistant", "content": "ï¾†ï½¬ï½°...ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸï¼‰"})

# ä¼šè©±ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãƒœã‚¿ãƒ³
if st.button("ä¼šè©±ã‚’ã‚¯ãƒªã‚¢"):
    st.session_state.messages = []
    st.experimental_rerun()
