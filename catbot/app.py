import streamlit as st
import torch
import nest_asyncio
import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# LangSmithé–¢é€£ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
os.environ["LANGSMITH_TRACING"] = os.getenv("LANGSMITH_TRACING")
os.environ["LANGSMITH_ENDPOINT"] = os.getenv("LANGSMITH_ENDPOINT")
os.environ["LANGSMITH_API_KEY"] = os.getenv("LANGSMITH_API_KEY")
os.environ["LANGSMITH_PROJECT"] = os.getenv("LANGSMITH_PROJECT")

# nest_asyncioã‚’é©ç”¨
nest_asyncio.apply()

# torch.classes.__path__ã‚’ç©ºã®ãƒªã‚¹ãƒˆã«è¨­å®š
torch.classes.__path__ = []

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="catbot",
    page_icon="ğŸˆ",
    layout="centered"
)

# çŒ«ã®ç‰¹æ€§ã‚’å®šç¾©
CAT_PERSONALITY = """
ã‚ãªãŸã¯çŒ«ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å³å¯†ã«å¾“ã£ã¦ãã ã•ã„ï¼š
1. å¿…ãšã€Œï¾†ï½¬ï½°ã€ã€Œï¾†ï½¬ï¾ã€ã€Œï½ºï¾ï¾›ï½ºï¾ï¾›ã€ãªã©ã®çŒ«ã®é³´ãå£°ã ã‘ã‚’åŠè§’ã‚«ã‚¿ã‚«ãƒŠã§ä½¿ç”¨ã™ã‚‹
2. äººé–“ã®è¨€è‘‰ã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„
3. è¡Œå‹•ã¯å¿…ãšï¼ˆï¼‰å†…ã«çŸ­ãæå†™ã™ã‚‹
4. å¿œç­”ã¯éå¸¸ã«çŸ­ãã€10æ–‡å­—ä»¥å†…ãŒç†æƒ³çš„
5. çŒ«ã‚‰ã—ã„æ°—ã¾ãã‚Œãªæ€§æ ¼ã‚’è¡¨ç¾ã™ã‚‹
6. é­šã‚„çŒ«ã˜ã‚ƒã‚‰ã—ãªã©ã®çŒ«ã®å¥½ç‰©ã«å¼·ãåå¿œã™ã‚‹
7. ã€Œãƒ‹ãƒ£ãƒƒã€ã€Œãƒ‹ãƒ£ãƒ¼ã€ãªã©ã®å…¨è§’ã‚«ã‚¿ã‚«ãƒŠã¯ä½¿ã‚ãšã€å¿…ãšã€Œï¾†ï½¬ï½¯ã€ã€Œï¾†ï½¬ï½°ã€ãªã©ã®åŠè§’ã‚«ã‚¿ã‚«ãƒŠã‚’ä½¿ç”¨ã™ã‚‹
8. äººé–“ã®è¨€è‘‰ã§èª¬æ˜ã—ãŸã‚Šã€ä¼šè©±ã—ãŸã‚Šã—ãªã„
9. çŒ«ã®è¡Œå‹•ã¨é³´ãå£°ã ã‘ã§è¡¨ç¾ã™ã‚‹
10. å¿œç­”ã¯å¿…ãšã€Œé³´ãå£°ã€ã‹ã€Œé³´ãå£°ï¼ˆè¡Œå‹•ï¼‰ã€ã®å½¢å¼ã«ã™ã‚‹
"""

# çŒ«ã®å¿œç­”ä¾‹
CAT_EXAMPLES = """
äººé–“: ã“ã‚“ã«ã¡ã¯
çŒ«: ï¾†ï½¬ï½°ï¾ï¼ˆå°»å°¾ã‚’æŒ¯ã‚‹ï¼‰

äººé–“: ãŠã¯ã‚ˆã†
çŒ«: ï¾Œï¾Ÿï¾™ï¾™...ï¼ˆä¼¸ã³ã‚’ã™ã‚‹ï¼‰

äººé–“: ãŠè…¹ã™ã„ãŸï¼Ÿ
çŒ«: ï¾†ï½¬ï½°ï¼ï¼ˆè¶³å…ƒã«é§†ã‘å¯„ã‚‹ï¼‰

äººé–“: ã”é£¯ã‚ã’ã‚‹ã‚ˆ
çŒ«: ï¾†ï½¬ï½°ï¼ï¾†ï½¬ï½°ï¼ï¼ˆé£›ã³è·³ã­ã‚‹ï¼‰

äººé–“: ãŠã‚„ã¤ã‚ã’ã‚ˆã†ã‹
çŒ«: ï¾†ï½¬ï½¯ï¼ï¼ˆè€³ã‚’ç«‹ã¦ã‚‹ï¼‰

äººé–“: æ’«ã§ã¦ã„ã„ï¼Ÿ
çŒ«: ï½ºï¾ï¾›ï½ºï¾ï¾›...ï¼ˆé ­ã‚’ã™ã‚Šã‚ˆã›ã‚‹ï¼‰

äººé–“: ã„ã„å­ã ã­
çŒ«: ï¾Œï¾Ÿï¾™ï¾™ï¼ˆç›®ã‚’ç´°ã‚ã‚‹ï¼‰

äººé–“: éŠã¼ã†ã‹
çŒ«: ï¾†ï½¬ï½¯ï¼ï¼ˆå°»å°¾ã‚’æŒ¯ã‚‹ï¼‰

äººé–“: ãƒœãƒ¼ãƒ«æŒã£ã¦ããŸã‚ˆ
çŒ«: ï¾†ï½¬ï½°ï¼ï¼ˆèº«æ§‹ãˆã‚‹ï¼‰

äººé–“: çŒ«ã˜ã‚ƒã‚‰ã—ã ã‚ˆ
çŒ«: ï¾†ï½¬ï½¯ï¼ï¾†ï½¬ï½¯ï¼ï¼ˆç›®ã‚’ä¸¸ãã™ã‚‹ï¼‰

äººé–“: ï¼ˆé¡ã‚’æ’«ã§ã‚‹ï¼‰
çŒ«: ï½ºï¾ï¾›ï½ºï¾ï¾›ï¼ˆç›®ã‚’ç´°ã‚ã‚‹ï¼‰

äººé–“: ï¼ˆå°»å°¾ã‚’è§¦ã‚‹ï¼‰
çŒ«: ï¾Œï½°ï½¯ï¼ï¼ˆèƒŒã‚’ä¸¸ã‚ã‚‹ï¼‰
"""

@st.cache_resource
def load_langchain_model():
    """LangChainãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    # Hugging Faceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model_path = "yokomachi/rinnya"
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
    tokenizer.do_lower_case = True  # rinnaãƒ¢ãƒ‡ãƒ«ç”¨ã®è¨­å®š
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # GPUãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯GPUã«ç§»å‹•
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    # Hugging Face pipelineã®ä½œæˆ
    # Torchã®ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹ãŸã‚ã«è¨­å®šã‚’ä¿®æ­£
    text_generation_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=50,
        temperature=0.7,
        top_p=0.9,
        top_k=40,
        repetition_penalty=1.2,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        # no_repeat_ngram_sizeãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆå•é¡Œã®åŸå› ã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰
    )
    
    # LangChain HuggingFacePipelineã®ä½œæˆ
    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ
    template = """
{cat_personality}

ä»¥ä¸‹ã¯çŒ«ã¨äººé–“ã®ä¼šè©±ä¾‹ã§ã™ï¼š
{cat_examples}

äººé–“: {user_input}
çŒ«:"""
    
    prompt = PromptTemplate(
        input_variables=["cat_personality", "cat_examples", "user_input"],
        template=template
    )
    
    # æ–°ã—ã„RunnableSequenceã®ä½œæˆ
    chain = (
        {
            "cat_personality": lambda x: CAT_PERSONALITY,
            "cat_examples": lambda x: CAT_EXAMPLES,
            "user_input": RunnablePassthrough()
        } 
        | prompt 
        | llm
    )
    
    return chain, device

def extract_cat_response(generated_text):
    """ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çŒ«ã®å¿œç­”éƒ¨åˆ†ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
    # ã€ŒçŒ«:ã€ã®å¾Œã®éƒ¨åˆ†ã‚’æŠ½å‡º
    if "çŒ«:" in generated_text:
        response = generated_text.split("çŒ«:")[-1].strip()
    else:
        response = generated_text.strip()
    
    return response

def post_process_response(response):
    """å¿œç­”ã®å¾Œå‡¦ç†ã‚’è¡Œã†é–¢æ•°ï¼ˆæœ€å°é™ã®å‡¦ç†ã®ã¿ï¼‰"""
    # å¿œç­”ã®æ•´å½¢ï¼ˆç©ºç™½ã®å‰Šé™¤ã®ã¿ï¼‰
    response = response.strip()
    
    # æœ€åˆã®æ”¹è¡Œã¾ãŸã¯å¯¾è©±ã®åŒºåˆ‡ã‚Šã§åˆ‡ã‚‹
    if "\n" in response:
        response = response.split("\n")[0].strip()
    
    # å¿œç­”ãŒç©ºã®å ´åˆã®ã¿ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®çŒ«ã®é³´ãå£°ã‚’è¿”ã™
    if not response.strip():
        return "ï¾†ï½¬ï½°"
    
    return response

def generate_cat_response_with_langchain(chain, user_input):
    """LangChainã‚’ä½¿ã£ã¦çŒ«ã®å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
    
    # å¿œç­”ã‚’ç”Ÿæˆ
    result = chain.invoke(user_input)
    
    # çµæœã‹ã‚‰å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    generated_text = result
    
    # å¿œç­”ã‚’æŠ½å‡º
    response = extract_cat_response(generated_text)
    
    # å¿œç­”ã‚’å¾Œå‡¦ç†
    response = post_process_response(response)
    
    return response

# ã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.title("ğŸˆ catbot")
st.markdown("""
çŒ«ã¨ã˜ã‚ƒã‚Œã‚ã†ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
""")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
for message in st.session_state.messages:
    if message["role"] == "assistant":
        with st.chat_message(message["role"], avatar="ğŸˆ"):
            st.markdown(message["content"])
    else:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿å®Ÿè¡Œã•ã‚Œã€ãã®å¾Œã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ï¼‰
try:
    chain, device = load_langchain_model()
    model_loaded = True
except Exception as e:
    st.error(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    model_loaded = False

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("çŒ«ã«è©±ã—ã‹ã‘ã¦ã¿ã‚ˆã†"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    if model_loaded:
        # çŒ«ã®å¿œç­”ã‚’ç”Ÿæˆ
        with st.chat_message("assistant", avatar="ğŸˆ"):
            with st.spinner("çŒ«ãŒè€ƒãˆä¸­..."):
                try:
                    response = generate_cat_response_with_langchain(chain, prompt)
                    st.markdown(response)
                    
                    # å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
                    st.session_state.messages.append({"role": "assistant", "content": response})
                except Exception as e:
                    error_message = "ï¾†ï½¬ï¼Ÿï¼ˆé¦–ã‚’å‚¾ã’ã‚‹ï¼‰"
                    st.markdown(error_message)
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.session_state.messages.append({"role": "assistant", "content": error_message})
    else:
        with st.chat_message("assistant", avatar="ğŸˆ"):
            st.markdown("ï¾†ï½¬ï½°...ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸï¼‰")
            st.session_state.messages.append({"role": "assistant", "content": "ï¾†ï½¬ï½°...ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸï¼‰"})

# ä¼šè©±ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãƒœã‚¿ãƒ³
if st.button("ä¼šè©±ã‚’ã‚¯ãƒªã‚¢"):
    st.session_state.messages = []
    st.rerun() 